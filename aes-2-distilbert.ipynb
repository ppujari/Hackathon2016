{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"},{"sourceId":7388005,"sourceType":"datasetVersion","datasetId":4294430},{"sourceId":8461984,"sourceType":"datasetVersion","datasetId":5044393},{"sourceId":8837812,"sourceType":"datasetVersion","datasetId":5318434},{"sourceId":32506,"sourceType":"modelInstanceVersion","modelInstanceId":27214},{"sourceId":58495,"sourceType":"modelInstanceVersion","modelInstanceId":49004},{"sourceId":63300,"sourceType":"modelInstanceVersion","modelInstanceId":52774}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":6235.795214,"end_time":"2024-06-09T09:36:54.551516","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-06-09T07:52:58.756302","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this competition, our aim is to develop an AI model that can score student essays. This competition is actually an updated version of an old one that took place over a decade ago. In this version, we aim to improve upon essay scoring algorithms to enhance student learning outcomes.\n\nGiven an essay with n words X = {xi}i=1 to n, we need to output one score y as a result of measuring the level of this essay.","metadata":{"papermill":{"duration":0.012003,"end_time":"2024-06-09T07:53:01.418137","exception":false,"start_time":"2024-06-09T07:53:01.406134","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport matplotlib as mpl\nimport nltk\ncmap = mpl.cm.get_cmap('coolwarm')\nimport torch\nimport torch.nn as nn\nimport re\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer, BertModel, BertTokenizer, BertConfig\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import cohen_kappa_score\nimport sys\nsys.path.append('/kaggle/input/py-file')\nsys.path.append('/kaggle/input/wordnet')\n\nfrom torch_shallow_neural_classifier import TorchShallowNeuralClassifier","metadata":{"papermill":{"duration":8.790156,"end_time":"2024-06-09T07:53:10.21984","exception":false,"start_time":"2024-06-09T07:53:01.429684","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T05:21:23.807051Z","iopub.execute_input":"2024-07-02T05:21:23.807530Z","iopub.status.idle":"2024-07-02T05:21:23.817111Z","shell.execute_reply.started":"2024-07-02T05:21:23.807495Z","shell.execute_reply":"2024-07-02T05:21:23.815458Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_33/1399127395.py:6: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n  cmap = mpl.cm.get_cmap('coolwarm')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# ⚙️ | Configuration","metadata":{"papermill":{"duration":0.0116,"end_time":"2024-06-09T07:53:10.24379","exception":false,"start_time":"2024-06-09T07:53:10.23219","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class CFG:\n    seed = 42  # Random seed \n    sequence_length = 512  # Input sequence length\n    batch_size = 16  # Batch size\n    #weights_name = \"/kaggle/input/bert-medium\"# Name of pretrained models\n    #weights_name = \"/kaggle/input/hf_bert-medium/pytorch/bert_medium/1\"\n    #weights_name = \"/kaggle/input/deberta-v3-small/transformers/v1/1\"\n    weights_name = \"/kaggle/input/distilbert/transformers/distilbert/1/distilB\"","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.018852,"end_time":"2024-06-09T07:53:10.275093","exception":false,"start_time":"2024-06-09T07:53:10.256241","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T05:21:23.819986Z","iopub.execute_input":"2024-07-02T05:21:23.820572Z","iopub.status.idle":"2024-07-02T05:21:23.833845Z","shell.execute_reply.started":"2024-07-02T05:21:23.820515Z","shell.execute_reply":"2024-07-02T05:21:23.832350Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# Fine-tune the model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available","metadata":{"papermill":{"duration":0.088803,"end_time":"2024-06-09T07:53:10.37539","exception":false,"start_time":"2024-06-09T07:53:10.286587","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T05:21:23.836454Z","iopub.execute_input":"2024-07-02T05:21:23.836903Z","iopub.status.idle":"2024-07-02T05:21:23.848295Z","shell.execute_reply.started":"2024-07-02T05:21:23.836869Z","shell.execute_reply":"2024-07-02T05:21:23.846849Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"# ♻️ | Reproducibility \nSets value for random seed to produce similar result in each run.","metadata":{"papermill":{"duration":0.011348,"end_time":"2024-06-09T07:53:10.399809","exception":false,"start_time":"2024-06-09T07:53:10.388461","status":"completed"},"tags":[]}},{"cell_type":"code","source":"torch.manual_seed(CFG.seed)\ntransformers.logging.set_verbosity_error()","metadata":{"papermill":{"duration":0.021761,"end_time":"2024-06-09T07:53:10.433928","exception":false,"start_time":"2024-06-09T07:53:10.412167","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T05:21:23.849662Z","iopub.execute_input":"2024-07-02T05:21:23.850146Z","iopub.status.idle":"2024-07-02T05:21:23.862881Z","shell.execute_reply.started":"2024-07-02T05:21:23.850105Z","shell.execute_reply":"2024-07-02T05:21:23.861377Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"# 📁 | Dataset Path ","metadata":{"papermill":{"duration":0.012939,"end_time":"2024-06-09T07:53:10.459482","exception":false,"start_time":"2024-06-09T07:53:10.446543","status":"completed"},"tags":[]}},{"cell_type":"code","source":"BASE_PATH = '/kaggle/input/learning-agency-lab-automated-essay-scoring-2'","metadata":{"papermill":{"duration":0.019538,"end_time":"2024-06-09T07:53:10.492376","exception":false,"start_time":"2024-06-09T07:53:10.472838","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T05:21:23.865424Z","iopub.execute_input":"2024-07-02T05:21:23.865870Z","iopub.status.idle":"2024-07-02T05:21:23.876329Z","shell.execute_reply.started":"2024-07-02T05:21:23.865835Z","shell.execute_reply":"2024-07-02T05:21:23.875081Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"# 📖 | Meta Data\n\n**Files in the dataset:**\n\n- `{test|train}.csv`\n  - `essay_id`: Unique identifier for each essay.\n  - `full_text`: Essay text.\n  - `score`: Essay's score from `1-6`.\n- `sample_submission.csv`: Valid sample submission.\n\n**What does the `score` mean?**\n\nThe `score` represents the quality of student-written argumentative essays. Essays were rated based on a rubric covering perspective development, critical thinking, evidence use, organization, language, and grammar/mechanics. Here's a summary of the scoring criteria:\n\n| Score | Description |\n|-------|-------------|\n| 6     | Clear mastery with few errors, outstanding critical thinking, appropriate evidence, well-organized, skilled language use. |\n| 5     | Reasonable mastery with occasional errors, strong critical thinking, generally appropriate evidence, well-organized, good language use. |\n| 4     | Adequate mastery with some lapses, competent critical thinking, adequate evidence, generally organized, fair language use. |\n| 3     | Developing mastery with weaknesses, limited critical thinking, inconsistent evidence, limited organization, fair language use with weaknesses. |\n| 2     | Little mastery with serious flaws, weak critical thinking, insufficient evidence, poor organization, limited language use with frequent errors. |\n| 1     | Very little or no mastery, severely flawed, no viable point of view, disorganized, fundamental language flaws, pervasive grammar/mechanics errors. |\n\n> This grading is very similar to the grading used in the [ETS GRE (Graduate Record Examinations) AWA](https://www.ets.org/gre/test-takers/general-test/prepare/content/analytical-writing.html) exam, where prospective graduate students are asked to write essays to judge their analytical abilities, and their scores are later used for graduate admission. \n","metadata":{"papermill":{"duration":0.011343,"end_time":"2024-06-09T07:53:10.51536","exception":false,"start_time":"2024-06-09T07:53:10.504017","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Load data\ndf = pd.read_csv(f'{BASE_PATH}/train.csv')  # Read CSV file into a DataFrame","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.73262,"end_time":"2024-06-09T07:53:11.259501","exception":false,"start_time":"2024-06-09T07:53:10.526881","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T05:21:23.878058Z","iopub.execute_input":"2024-07-02T05:21:23.878901Z","iopub.status.idle":"2024-07-02T05:21:24.337108Z","shell.execute_reply.started":"2024-07-02T05:21:23.878864Z","shell.execute_reply":"2024-07-02T05:21:24.335913Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# Display information about the train data\nprint(\"# Train Data: {:,}\".format(len(df)))\n# Set the display options to show the maximum column width\npd.set_option('display.max_colwidth', None)\ndf.head(2)","metadata":{"papermill":{"duration":0.030812,"end_time":"2024-06-09T07:53:11.30365","exception":false,"start_time":"2024-06-09T07:53:11.272838","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T05:21:24.338526Z","iopub.execute_input":"2024-07-02T05:21:24.338970Z","iopub.status.idle":"2024-07-02T05:21:24.352473Z","shell.execute_reply.started":"2024-07-02T05:21:24.338931Z","shell.execute_reply":"2024-07-02T05:21:24.351343Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"# Train Data: 17,307\n","output_type":"stream"},{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"  essay_id  \\\n0  000d118   \n1  000fe60   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               full_text  \\\n0  Many people have car where they live. The thing they don't know is that when you use a car alot of thing can happen like you can get in accidet or the smoke that the car has is bad to breath on if someone is walk but in VAUBAN,Germany they dont have that proble because 70 percent of vauban's families do not own cars,and 57 percent sold a car to move there. Street parkig ,driveways and home garages are forbidden on the outskirts of freiburd that near the French and Swiss borders. You probaly won't see a car in Vauban's streets because they are completely \"car free\" but If some that lives in VAUBAN that owns a car ownership is allowed,but there are only two places that you can park a large garages at the edge of the development,where a car owner buys a space but it not cheap to buy one they sell the space for you car for $40,000 along with a home. The vauban people completed this in 2006 ,they said that this an example of a growing trend in Europe,The untile states and some where else are suburban life from auto use this is called \"smart planning\". The current efforts to drastically reduce greenhouse gas emissions from tailes the passengee cars are responsible for 12 percent of greenhouse gas emissions in Europe and up to 50 percent in some car intensive in the United States. I honeslty think that good idea that they did that is Vaudan because that makes cities denser and better for walking and in VAUBAN there are 5,500 residents within a rectangular square mile. In the artical David Gold berg said that \"All of our development since World war 2 has been centered on the cars,and that will have to change\" and i think that was very true what David Gold said because alot thing we need cars to do we can go anyway were with out cars beacuse some people are a very lazy to walk to place thats why they alot of people use car and i think that it was a good idea that that they did that in VAUBAN so people can see how we really don't need car to go to place from place because we can walk from were we need to go or we can ride bycles with out the use of a car. It good that they are doing that if you thik about your help the earth in way and thats a very good thing to. In the United states ,the Environmental protection Agency is promoting what is called \"car reduced\"communtunties,and the legislators are starting to act,if cautiously. Maany experts expect pubic transport serving suburbs to play a much larger role in a new six years federal transportation bill to approved this year. In previous bill,80 percent of appropriations have by law gone to highways and only 20 percent to other transports. There many good reason why they should do this.       \n1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          I am a scientist at NASA that is discussing the \"face\" on mars. I will be explaining how the \"face\" is a land form. By sharing my information about this isue i will tell you just that.\\n\\nFirst off, how could it be a martions drawing. There is no plant life on mars as of rite now that we know of, which means so far as we know it is not possible for any type of life. That explains how it could not be made by martians. Also why and how would a martion build a face so big. It just does not make any since that a martian did this.\\n\\nNext, why it is a landform. There are many landforms that are weird here in America, and there is also landforms all around the whole Earth. Many of them look like something we can relate to like a snake a turtle a human... So if there are landforms on earth dont you think landforms are on mars to? Of course! why not? It's just unique that the landform on Mars looks like a human face. Also if there was martians and they were trying to get our attention dont you think we would have saw one by now?\\n\\nFinaly, why you should listen to me. You should listen to me because i am a member of NASA and i've been dealing with all of this stuff that were talking about and people who say martians did this have no relation with NASA and have never worked with anything to relate to this landform. One last thing is that everyone working at NASA says the same thing i say, that the \"face\" is just a landform.\\n\\nTo sum all this up the \"face\" on mars is a landform but others would like to beleive it's a martian sculpture. Which every one that works at NASA says it's a landform and they are all the ones working on the planet and taking pictures.   \n\n   score  \n0      3  \n1      3  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>essay_id</th>\n      <th>full_text</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000d118</td>\n      <td>Many people have car where they live. The thing they don't know is that when you use a car alot of thing can happen like you can get in accidet or the smoke that the car has is bad to breath on if someone is walk but in VAUBAN,Germany they dont have that proble because 70 percent of vauban's families do not own cars,and 57 percent sold a car to move there. Street parkig ,driveways and home garages are forbidden on the outskirts of freiburd that near the French and Swiss borders. You probaly won't see a car in Vauban's streets because they are completely \"car free\" but If some that lives in VAUBAN that owns a car ownership is allowed,but there are only two places that you can park a large garages at the edge of the development,where a car owner buys a space but it not cheap to buy one they sell the space for you car for $40,000 along with a home. The vauban people completed this in 2006 ,they said that this an example of a growing trend in Europe,The untile states and some where else are suburban life from auto use this is called \"smart planning\". The current efforts to drastically reduce greenhouse gas emissions from tailes the passengee cars are responsible for 12 percent of greenhouse gas emissions in Europe and up to 50 percent in some car intensive in the United States. I honeslty think that good idea that they did that is Vaudan because that makes cities denser and better for walking and in VAUBAN there are 5,500 residents within a rectangular square mile. In the artical David Gold berg said that \"All of our development since World war 2 has been centered on the cars,and that will have to change\" and i think that was very true what David Gold said because alot thing we need cars to do we can go anyway were with out cars beacuse some people are a very lazy to walk to place thats why they alot of people use car and i think that it was a good idea that that they did that in VAUBAN so people can see how we really don't need car to go to place from place because we can walk from were we need to go or we can ride bycles with out the use of a car. It good that they are doing that if you thik about your help the earth in way and thats a very good thing to. In the United states ,the Environmental protection Agency is promoting what is called \"car reduced\"communtunties,and the legislators are starting to act,if cautiously. Maany experts expect pubic transport serving suburbs to play a much larger role in a new six years federal transportation bill to approved this year. In previous bill,80 percent of appropriations have by law gone to highways and only 20 percent to other transports. There many good reason why they should do this.</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000fe60</td>\n      <td>I am a scientist at NASA that is discussing the \"face\" on mars. I will be explaining how the \"face\" is a land form. By sharing my information about this isue i will tell you just that.\\n\\nFirst off, how could it be a martions drawing. There is no plant life on mars as of rite now that we know of, which means so far as we know it is not possible for any type of life. That explains how it could not be made by martians. Also why and how would a martion build a face so big. It just does not make any since that a martian did this.\\n\\nNext, why it is a landform. There are many landforms that are weird here in America, and there is also landforms all around the whole Earth. Many of them look like something we can relate to like a snake a turtle a human... So if there are landforms on earth dont you think landforms are on mars to? Of course! why not? It's just unique that the landform on Mars looks like a human face. Also if there was martians and they were trying to get our attention dont you think we would have saw one by now?\\n\\nFinaly, why you should listen to me. You should listen to me because i am a member of NASA and i've been dealing with all of this stuff that were talking about and people who say martians did this have no relation with NASA and have never worked with anything to relate to this landform. One last thing is that everyone working at NASA says the same thing i say, that the \"face\" is just a landform.\\n\\nTo sum all this up the \"face\" on mars is a landform but others would like to beleive it's a martian sculpture. Which every one that works at NASA says it's a landform and they are all the ones working on the planet and taking pictures.</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"count = (df['score'] == 1).sum()\nprint(count)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:21:24.356151Z","iopub.execute_input":"2024-07-02T05:21:24.356627Z","iopub.status.idle":"2024-07-02T05:21:24.364835Z","shell.execute_reply.started":"2024-07-02T05:21:24.356588Z","shell.execute_reply":"2024-07-02T05:21:24.363418Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"1252\n","output_type":"stream"}]},{"cell_type":"code","source":"# Ensure the necessary nltk data is downloaded\nnltk.download('wordnet')\nnltk.download('omw-1.4')  # Optional: for additional WordNet resources\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:21:24.366592Z","iopub.execute_input":"2024-07-02T05:21:24.367114Z","iopub.status.idle":"2024-07-02T05:21:24.447113Z","shell.execute_reply.started":"2024-07-02T05:21:24.367038Z","shell.execute_reply":"2024-07-02T05:21:24.445809Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n","output_type":"stream"},{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"def f7(seq):\n    \"\"\"\n    Makes a list unique\n    \"\"\"\n    seen = set()\n    seen_add = seen.add\n    return [x for x in seq if x not in seen and not seen_add(x)]\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:21:24.448847Z","iopub.execute_input":"2024-07-02T05:21:24.449210Z","iopub.status.idle":"2024-07-02T05:21:24.456527Z","shell.execute_reply.started":"2024-07-02T05:21:24.449179Z","shell.execute_reply":"2024-07-02T05:21:24.455088Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import wordnet\n\ndef get_wordnet_syns(word):\n    \"\"\"\n    Utilize wordnet (installed with nltk) to get synonyms for words\n    word is the input word\n    returns a list of unique synonyms\n    \"\"\"\n    synonyms = []\n    regex = r\"_\"\n    pat = re.compile(regex)\n    synset = wordnet.synsets(word)\n    for ss in synset:\n        for swords in ss.lemma_names():\n            synonyms.append(pat.sub(\" \", swords.lower()))\n    synonyms = f7(synonyms)\n    return synonyms","metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:22:40.951984Z","iopub.execute_input":"2024-07-02T05:22:40.952877Z","iopub.status.idle":"2024-07-02T05:22:40.960515Z","shell.execute_reply.started":"2024-07-02T05:22:40.952839Z","shell.execute_reply":"2024-07-02T05:22:40.959268Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"def replace_first_zero_with_variable(s, replacement):\n    # Find the first occurrence of '0'\n    index = s.find('0')\n    if index != -1:\n        # Replace the first '0' with the replacement variable\n        s = s[:index] + replacement + s[index+1:]\n    return s","metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:21:24.469754Z","iopub.execute_input":"2024-07-02T05:21:24.471910Z","iopub.status.idle":"2024-07-02T05:21:24.484266Z","shell.execute_reply.started":"2024-07-02T05:21:24.471857Z","shell.execute_reply":"2024-07-02T05:21:24.482813Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"def generate_additional_essays(row, dictionary=None, max_syns=3):\n        \"\"\"\n        Substitute synonyms to generate extra essays from existing ones.\n        This is done to increase the amount of training data.\n        Should only be used with lowest scoring essays.\n        e_text is the text of the original essay.\n        e_score is the score of the original essay.\n        dictionary is a fixed dictionary (list) of words to replace.\n        max_syns defines the maximum number of additional essays to generate.  Do not set too high.\n        \"\"\"\n        essay_id = row['essay_id'] \n        e_text = row['full_text']\n        e_score = row['score']\n        if e_score < 5:\n            print('returning')\n            return\n        e_toks = nltk.word_tokenize(e_text)\n        all_syns = []\n        for word in e_toks:\n            synonyms = get_wordnet_syns(word)\n            if(len(synonyms) > max_syns):\n                synonyms = random.sample(synonyms, max_syns)\n            all_syns.append(synonyms)\n        new_essays = []\n        for i in range(0, max_syns):\n            syn_toks = e_toks\n            for z in range(0, len(e_toks)):\n                if len(all_syns[z]) > i and (dictionary == None or e_toks[z] in dictionary):\n                    syn_toks[z] = all_syns[z][i]\n            new_essays.append(\" \".join(syn_toks))\n        for z in range(0, len(new_essays)):\n            modified_string = replace_first_zero_with_variable(essay_id, z)\n            # New record to insert\n            new_record = {'essay_id':modified_string,'full_text':new_essays[z],'score': e_score}\n\n            # Append the new record\n            df = df.append(new_record, ignore_index=True)\n        return df","metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:21:24.486197Z","iopub.execute_input":"2024-07-02T05:21:24.486713Z","iopub.status.idle":"2024-07-02T05:21:24.504554Z","shell.execute_reply.started":"2024-07-02T05:21:24.486644Z","shell.execute_reply":"2024-07-02T05:21:24.503147Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"df_new['new_column'] = df.apply(generate_additional_essays, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:22:46.187194Z","iopub.execute_input":"2024-07-02T05:22:46.187603Z","iopub.status.idle":"2024-07-02T05:22:46.478271Z","shell.execute_reply.started":"2024-07-02T05:22:46.187573Z","shell.execute_reply":"2024-07-02T05:22:46.476644Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"returning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\nreturning\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:80\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzip_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m e\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/data.py:653\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    652\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (sep, msg, sep)\n\u001b[0;32m--> 653\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/wordnet.zip/wordnet/.zip/' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_new[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnew_column\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerate_additional_essays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10373\u001b[0m )\n\u001b[0;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[0;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n","Cell \u001b[0;32mIn[50], line 20\u001b[0m, in \u001b[0;36mgenerate_additional_essays\u001b[0;34m(row, dictionary, max_syns)\u001b[0m\n\u001b[1;32m     18\u001b[0m all_syns \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m e_toks:\n\u001b[0;32m---> 20\u001b[0m     synonyms \u001b[38;5;241m=\u001b[39m \u001b[43mget_wordnet_syns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(synonyms) \u001b[38;5;241m>\u001b[39m max_syns):\n\u001b[1;32m     22\u001b[0m         synonyms \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(synonyms, max_syns)\n","Cell \u001b[0;32mIn[52], line 11\u001b[0m, in \u001b[0;36mget_wordnet_syns\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m      9\u001b[0m regex \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m pat \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(regex)\n\u001b[0;32m---> 11\u001b[0m synset \u001b[38;5;241m=\u001b[39m \u001b[43mwordnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynsets\u001b[49m(word)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ss \u001b[38;5;129;01min\u001b[39;00m synset:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m swords \u001b[38;5;129;01min\u001b[39;00m ss\u001b[38;5;241m.\u001b[39mlemma_names():\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:116\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir, zip_name))\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:78\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir, zip_name))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/data.py:653\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    651\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    652\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (sep, msg, sep)\n\u001b[0;32m--> 653\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************"],"ename":"LookupError","evalue":"\n**********************************************************************\n  Resource 'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************","output_type":"error"}]},{"cell_type":"code","source":"df['essay_length'] = df['full_text'].str.split().str.len()","metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:21:24.709556Z","iopub.status.idle":"2024-07-02T05:21:24.710018Z","shell.execute_reply.started":"2024-07-02T05:21:24.709810Z","shell.execute_reply":"2024-07-02T05:21:24.709829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we set ourselves up to use BERT-mini:","metadata":{"papermill":{"duration":0.011588,"end_time":"2024-06-09T07:53:11.327259","exception":false,"start_time":"2024-06-09T07:53:11.315671","status":"completed"},"tags":[]}},{"cell_type":"code","source":"bert = AutoModel.from_pretrained(CFG.weights_name)\n\nbert_tokenizer = AutoTokenizer.from_pretrained(CFG.weights_name)","metadata":{"papermill":{"duration":1.853905,"end_time":"2024-06-09T07:53:13.193137","exception":false,"start_time":"2024-06-09T07:53:11.339232","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T05:21:24.712880Z","iopub.status.idle":"2024-07-02T05:21:24.713484Z","shell.execute_reply.started":"2024-07-02T05:21:24.713186Z","shell.execute_reply":"2024-07-02T05:21:24.713211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_segments(text, first_n=382, last_n=128):\n    \"\"\"\n    Extracts the first `first_n` words and the last `last_n` words from the text if the text length is >= first_n + last_n.\n    Otherwise, returns the text as is.\n\n    Args:\n        text (str): The input text.\n        first_n (int): The number of words to take from the beginning.\n        last_n (int): The number of words to take from the end.\n\n    Returns:\n        str: The extracted segments or the original text.\n    \"\"\"\n    # Split the text into words\n    words = text.split()\n\n    # Check if the text length is at least first_n + last_n\n    if len(words) > (first_n + last_n):\n        # Extract the first first_n words and the last last_n words\n        extracted_words = words[:first_n] + words[-last_n:]\n    else:\n        # Return the entire text as is\n        extracted_words = words\n\n    # Join the words back into a single string\n    return \" \".join(extracted_words)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:21:24.715272Z","iopub.status.idle":"2024-07-02T05:21:24.715960Z","shell.execute_reply.started":"2024-07-02T05:21:24.715614Z","shell.execute_reply":"2024-07-02T05:21:24.715638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def to_ordinal(y, num_classes=None, dtype=\"int\"):\n    \"\"\"Converts a class vector (integers) to an ordinal regression matrix.\n\n    This utility encodes class vector to ordinal regression/classification\n    matrix where each sample is indicated by a row and rank of that sample is\n    indicated by number of ones in that row.\n\n    Args:\n        y: Array-like with class values to be converted into a matrix\n            (integers from 0 to `num_classes - 1`).\n        num_classes: Total number of classes. If `None`, this would be inferred\n            as `max(y) + 1`.\n        dtype: The data type expected by the input. Default: `'float32'`.\n\n    Returns:\n        An ordinal regression matrix representation of the input as a NumPy\n        array. The class axis is placed last.\n    \"\"\"\n    y = np.array(y, dtype=\"int\")\n    input_shape = y.shape\n\n    # Shrink the last dimension if the shape is (..., 1).\n    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n        input_shape = tuple(input_shape[:-1])\n\n    y = y.reshape(-1)\n    if not num_classes:\n        num_classes = np.max(y) + 1\n    n = y.shape[0]\n    range_values = np.arange(num_classes - 1)\n    range_values = np.tile(np.expand_dims(range_values, 0), [n, 1])\n    ordinal = np.zeros((n, num_classes - 1), dtype=dtype)\n    ordinal[range_values < np.expand_dims(y, -1)] = 1\n    output_shape = input_shape + (num_classes - 1,)\n    ordinal = np.reshape(ordinal, output_shape)\n    return ordinal","metadata":{"papermill":{"duration":0.023548,"end_time":"2024-06-09T07:53:13.548668","exception":false,"start_time":"2024-06-09T07:53:13.52512","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T05:21:24.718346Z","iopub.status.idle":"2024-07-02T05:21:24.718976Z","shell.execute_reply.started":"2024-07-02T05:21:24.718646Z","shell.execute_reply":"2024-07-02T05:21:24.718672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Label Conversion","metadata":{"papermill":{"duration":0.012224,"end_time":"2024-06-09T07:53:13.573346","exception":false,"start_time":"2024-06-09T07:53:13.561122","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df['label'] = to_ordinal(df.score.values).tolist()","metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:21:24.721499Z","iopub.status.idle":"2024-07-02T05:21:24.722121Z","shell.execute_reply.started":"2024-07-02T05:21:24.721806Z","shell.execute_reply":"2024-07-02T05:21:24.721832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):\n    if text is None:\n        print(\"Error: Received None text input.\")\n        return []\n\n    # Remove '@name'\n    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n    # Replace '&amp;' with '&'\n    text = re.sub(r'&amp;', '&', text)\n    # Remove trailing whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n    text = text.lower()\n    # Replace \"\\n\\n\" with a single space\n    cleaned_text = re.sub(r'\\n\\n', ' ', text)\n    # Replace \"\\'\" with \"'\"\n    cleaned_text = re.sub(r\"\\\\'\", \"'\", cleaned_text)\n    \n    return extract_segments(cleaned_text)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:21:24.724140Z","iopub.status.idle":"2024-07-02T05:21:24.724767Z","shell.execute_reply.started":"2024-07-02T05:21:24.724452Z","shell.execute_reply":"2024-07-02T05:21:24.724475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sub_chars(string):\n    \"\"\"\n    Strips illegal characters from a string.  Used to sanitize input essays.\n    Removes all non-punctuation, digit, or letter characters.\n    Returns sanitized string.\n    string - string\n    \"\"\"\n    #Define replacement patterns\n    sub_pat = r\"[^A-Za-z\\.\\?!,';:]\"\n    char_pat = r\"\\.\"\n    com_pat = r\",\"\n    ques_pat = r\"\\?\"\n    excl_pat = r\"!\"\n    sem_pat = r\";\"\n    col_pat = r\":\"\n    whitespace_pat = r\"\\s{1,}\"\n\n    #Replace text.  Ordering is very important!\n    nstring = re.sub(sub_pat, \" \", string)\n    nstring = re.sub(char_pat,\" .\", nstring)\n    nstring = re.sub(com_pat, \" ,\", nstring)\n    nstring = re.sub(ques_pat, \" ?\", nstring)\n    nstring = re.sub(excl_pat, \" !\", nstring)\n    nstring = re.sub(sem_pat, \" ;\", nstring)\n    nstring = re.sub(col_pat, \" :\", nstring)\n    nstring = re.sub(whitespace_pat, \" \", nstring)\n\n    return nstring","metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:21:24.726534Z","iopub.status.idle":"2024-07-02T05:21:24.727137Z","shell.execute_reply.started":"2024-07-02T05:21:24.726845Z","shell.execute_reply":"2024-07-02T05:21:24.726868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['cleaned_full_text'] = df['full_text'].apply(lambda x:clean_text(x))","metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:21:24.729136Z","iopub.status.idle":"2024-07-02T05:21:24.729750Z","shell.execute_reply.started":"2024-07-02T05:21:24.729446Z","shell.execute_reply":"2024-07-02T05:21:24.729469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 🔪 | Data Split\n\nIn the code snippet provided below, we will divide the existing **train** data into folds using a stratification of `label` column.","metadata":{"execution":{"iopub.status.busy":"2024-06-17T00:54:35.953423Z","iopub.execute_input":"2024-06-17T00:54:35.954317Z","iopub.status.idle":"2024-06-17T00:54:35.96003Z","shell.execute_reply.started":"2024-06-17T00:54:35.954284Z","shell.execute_reply":"2024-06-17T00:54:35.958962Z"}}},{"cell_type":"code","source":"train_df, valid_df = train_test_split(df, test_size=0.2, stratify=df[\"score\"], shuffle=True, random_state=CFG.seed)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:21:24.731422Z","iopub.status.idle":"2024-07-02T05:21:24.732022Z","shell.execute_reply.started":"2024-07-02T05:21:24.731702Z","shell.execute_reply":"2024-07-02T05:21:24.731747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(2)","metadata":{"papermill":{"duration":0.026155,"end_time":"2024-06-09T07:53:13.654416","exception":false,"start_time":"2024-06-09T07:53:13.628261","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T05:21:24.734740Z","iopub.status.idle":"2024-07-02T05:21:24.735374Z","shell.execute_reply.started":"2024-07-02T05:21:24.735048Z","shell.execute_reply":"2024-07-02T05:21:24.735073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 🎨 |Data Visualization","metadata":{"papermill":{"duration":0.012962,"end_time":"2024-06-09T07:53:13.680352","exception":false,"start_time":"2024-06-09T07:53:13.66739","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Show distribution of answers using a bar plot\nplt.figure(figsize=(8, 4))\ndf.score.value_counts().plot.bar(color=[cmap(0.0), cmap(0.25), cmap(0.65), cmap(0.9), cmap(1.0)])\nplt.xlabel(\"Score\")\nplt.ylabel(\"Count\")\nplt.title(\"Score distribution for Train Data\")\nplt.show()\n\n# Show distribution of essay length using a bar plot\nplt.figure(figsize=(8, 4))\ndf['essay_length'] = df.full_text.map(len)\ndf.essay_length.plot.hist(logy=False, color=cmap(0.9))\nplt.xlabel(\"Essay Length\")\nplt.ylabel(\"Count\")\nplt.title(\"Essay Length distribution for Train Data\")\nplt.show()","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.615066,"end_time":"2024-06-09T07:53:14.308364","exception":false,"start_time":"2024-06-09T07:53:13.693298","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T05:21:24.737198Z","iopub.status.idle":"2024-07-02T05:21:24.737871Z","shell.execute_reply.started":"2024-07-02T05:21:24.737496Z","shell.execute_reply":"2024-07-02T05:21:24.737522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 🍽️ | Preprocessing\n\n**What it does:** The preprocessor takes input strings and transforms them into a dictionary (`token_ids`, `padding_mask`) containing preprocessed tensors. This process starts with tokenization, where input strings are converted into sequences of token IDs.\n\n**Why it's important:** Initially, raw text data is complex and challenging for modeling due to its high dimensionality. By converting text into a compact set of tokens, such as transforming `\"The quick brown fox\"` into `[\"the\", \"qu\", \"##ick\", \"br\", \"##own\", \"fox\"]`, we simplify the data. Many models rely on special tokens and additional tensors to understand input. These tokens help divide input and identify padding, among other tasks. Making all sequences the same length through padding boosts computational efficiency, making subsequent steps smoother.","metadata":{"papermill":{"duration":0.013573,"end_time":"2024-06-09T07:53:14.33604","exception":false,"start_time":"2024-06-09T07:53:14.322467","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Batch tokenization\nuse the batch_encode_plus method for bert_tokenizer to tokenize a list of strings.","metadata":{"papermill":{"duration":0.013645,"end_time":"2024-06-09T07:53:14.363387","exception":false,"start_time":"2024-06-09T07:53:14.349742","status":"completed"},"tags":[]}},{"cell_type":"code","source":" def get_batch_token_ids(batch, tokenizer):\n    \"\"\"Map `batch` to a tensor of ids. The return\n    value should meet the following specification:\n\n    1. The max length should be 512.\n    2. Examples longer than the max length should be truncated\n    3. Examples should be padded to the max length for the batch.\n    4. The special [CLS] should be added to the start and the special\n       token [SEP] should be added to the end.\n    5. The attention mask should be returned\n    6. The return value of each component should be a tensor.\n\n    Parameters\n    ----------\n    batch: list of str\n    tokenizer: Hugging Face tokenizer\n\n    Returns\n    -------\n    dict with at least \"input_ids\" and \"attention_mask\" as keys,\n    each with Tensor values\n\n    \"\"\"\n    encoding = tokenizer.batch_encode_plus(batch, max_length=CFG.sequence_length, padding='max_length',\n                                     truncation=True, return_tensors='pt', add_special_tokens=True)\n\n    return encoding\n","metadata":{"papermill":{"duration":0.022573,"end_time":"2024-06-09T07:53:14.438503","exception":false,"start_time":"2024-06-09T07:53:14.41593","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T05:21:24.739674Z","iopub.status.idle":"2024-07-02T05:21:24.740312Z","shell.execute_reply.started":"2024-07-02T05:21:24.739994Z","shell.execute_reply":"2024-07-02T05:21:24.740026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fine-tuning module\n1. in the init method, define self.classifier_layer using nn.Sequential\n2. Complete the forward method.","metadata":{"papermill":{"duration":0.013599,"end_time":"2024-06-09T07:53:14.466083","exception":false,"start_time":"2024-06-09T07:53:14.452484","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class BertClassifierModule(nn.Module):\n    def __init__(self,\n            n_classes,\n            hidden_activation,\n            weights_name=CFG.weights_name):\n        \"\"\"This module loads a Transformer based on  `weights_name`,\n        puts it in train mode, add a dense layer with activation\n        function give by `hidden_activation`, and puts a classifier\n        layer on top of that as the final output. The output of\n        the dense layer should have the same dimensionality as the\n        model input.\n\n        Parameters\n        ----------\n        n_classes : int\n            Number of classes for the output layer\n        hidden_activation : torch activation function\n            e.g., nn.Tanh()\n        weights_name : str\n            Name of pretrained model to load from Hugging Face\n\n        \"\"\"\n        super().__init__()\n        self.n_classes = n_classes\n        self.weights_name = CFG.weights_name\n        self.bert = AutoModel.from_pretrained(self.weights_name)\n        self.bert.train()\n        self.hidden_activation = hidden_activation\n        self.hidden_dim = self.bert.embeddings.word_embeddings.embedding_dim\n        # Add the new parameters here using `nn.Sequential`.\n        # We can define this layer as\n        #\n        #  h = f(cW1 + b_h)\n        #  y = hW2 + b_y\n        #\n        # where c is the final hidden state above the [CLS] token,\n        # W1 has dimensionality (self.hidden_dim, self.hidden_dim),\n        # W2 has dimensionality (self.hidden_dim, self.n_classes),\n        # f is the hidden activation, and we rely on the PyTorch loss\n        # function to add apply a softmax to y.\n        self.classifier_layer = None\n        ##### YOUR CODE HERE\n       # Define the classifier_layer using nn.Sequential\n        \n        self.classifier_layer = nn.Sequential(\n            nn.Linear(self.hidden_dim,self.hidden_dim),  \n            self.hidden_activation,      # Activation function\n            nn.Dropout(0.1),\n            nn.Linear(self.hidden_dim, self.n_classes)\n        )\n        #self.classifier_layer.apply(init_weights)\n        \n    def forward(self, indices, mask):\n        \"\"\"Process `indices` with `mask` by feeding these arguments\n        to `self.bert` and then feeding the initial hidden state\n        in `last_hidden_state` to `self.classifier_layer`\n\n        Parameters\n        ----------\n        indices : tensor.LongTensor of shape (n_batch, k)\n            Indices into the `self.bert` embedding layer. `n_batch` is\n            the number of examples and `k` is the sequence length for\n            this batch\n        mask : tensor.LongTensor of shape (n_batch, d)\n            Binary vector indicating which values should be masked.\n            `n_batch` is the number of examples and `k` is the\n            sequence length for this batch\n\n        Returns\n        -------\n        tensor.FloatTensor\n            Predicted values, shape `(n_batch, self.n_classes)`\n\n        \"\"\"\n        # Process indices and mask through self.bert\n        outputs = self.bert(indices, attention_mask=mask)\n\n        # Extract the [CLS] token representation\n        #cls_token_representation = outputs.last_hidden_state[:, 0, :]\n        # Extract all token representations\n        last_hidden_state = outputs.last_hidden_state\n\n        # Apply max-pooling across the sequence dimension (dim=1)\n        cls_token_representation, _ = torch.max(last_hidden_state, dim=1)\n\n        # Apply the classifier layer\n        logits = self.classifier_layer(cls_token_representation)\n        return logits","metadata":{"papermill":{"duration":0.027384,"end_time":"2024-06-09T07:53:14.507455","exception":false,"start_time":"2024-06-09T07:53:14.480071","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T05:21:24.743427Z","iopub.status.idle":"2024-07-02T05:21:24.744057Z","shell.execute_reply.started":"2024-07-02T05:21:24.743747Z","shell.execute_reply":"2024-07-02T05:21:24.743771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_module = BertClassifierModule(n_classes=6, hidden_activation=nn.Tanh())","metadata":{"papermill":{"duration":0.273958,"end_time":"2024-06-09T07:53:14.795287","exception":false,"start_time":"2024-06-09T07:53:14.521329","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T05:21:24.745968Z","iopub.status.idle":"2024-07-02T05:21:24.746525Z","shell.execute_reply.started":"2024-07-02T05:21:24.746244Z","shell.execute_reply":"2024-07-02T05:21:24.746267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BertClassifier(TorchShallowNeuralClassifier):\n    def __init__(self, weights_name, *args, **kwargs):\n        self.weights_name = CFG.weights_name\n        self.tokenizer = AutoTokenizer.from_pretrained(self.weights_name)\n        super().__init__(*args, **kwargs)\n        self.params += ['weights_name']\n        self.classes_ = None\n        self.n_classes_ = 6\n        \n    def build_graph(self):\n        return BertClassifierModule(\n            self.n_classes_, self.hidden_activation, self.weights_name)\n\n    def build_dataset(self, X, y=None):\n        data = get_batch_token_ids(X, self.tokenizer)\n        if y is None:\n            dataset = torch.utils.data.TensorDataset(\n                data['input_ids'], data['attention_mask'])\n        else:\n            self.classes_ = np.unique(y, axis=0)\n            self.n_classes_ = self.classes_.shape[1]\n            \n            y = np.array(y, dtype=np.float32)\n            y_tensor = torch.tensor(y, dtype=torch.float32)\n            \n            dataset = torch.utils.data.TensorDataset(\n                data['input_ids'], data['attention_mask'], y_tensor)\n        return dataset\n","metadata":{"papermill":{"duration":0.025243,"end_time":"2024-06-09T07:53:14.834908","exception":false,"start_time":"2024-06-09T07:53:14.809665","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T05:21:24.748036Z","iopub.status.idle":"2024-07-02T05:21:24.748648Z","shell.execute_reply.started":"2024-07-02T05:21:24.748328Z","shell.execute_reply":"2024-07-02T05:21:24.748352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_finetune = BertClassifier(\n    weights_name=CFG.weights_name,\n    hidden_activation=nn.GELU(),\n    hidden_dim=512,\n    max_iter=1,\n    eta=3e-5,             # Low learning rate for effective fine-tuning.\n    batch_size=CFG.batch_size,         # Small batches to avoid memory overload.\n    gradient_accumulation_steps=4,  # Increase the effective batch size to 32.\n    early_stopping=True,  # Early-stopping\n    n_iter_no_change=5)   # params","metadata":{"papermill":{"duration":0.081917,"end_time":"2024-06-09T07:53:14.931127","exception":false,"start_time":"2024-06-09T07:53:14.84921","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T05:21:24.750302Z","iopub.status.idle":"2024-07-02T05:21:24.750931Z","shell.execute_reply.started":"2024-07-02T05:21:24.750579Z","shell.execute_reply":"2024-07-02T05:21:24.750603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_labels = train_df.label.to_list() # Extract training labels\n_ = bert_finetune.fit(train_df['cleaned_full_text'].to_list(),train_labels)","metadata":{"papermill":{"duration":6169.994357,"end_time":"2024-06-09T09:36:04.974018","exception":false,"start_time":"2024-06-09T07:53:14.979661","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T05:21:24.752738Z","iopub.status.idle":"2024-07-02T05:21:24.753292Z","shell.execute_reply.started":"2024-07-02T05:21:24.753016Z","shell.execute_reply":"2024-07-02T05:21:24.753039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = bert_finetune.predict(valid_df['cleaned_full_text'].tolist())","metadata":{"papermill":{"duration":46.353836,"end_time":"2024-06-09T09:36:51.344843","exception":false,"start_time":"2024-06-09T09:36:04.991007","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T05:21:24.755751Z","iopub.status.idle":"2024-07-02T05:21:24.756317Z","shell.execute_reply.started":"2024-07-02T05:21:24.756036Z","shell.execute_reply":"2024-07-02T05:21:24.756058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 📏 | Metric\n\nThe metric for this competition is quadratic **Weighted Kappa**. This metric is particularly useful for tasks involving ordinal classification (where labels have inherent order). The following code implements this metric from scratch. This metric is implemented taking inspiration from [this TensorFlow implementation](https://www.tensorflow.org/addons/api_docs/python/tfa/losses/WeightedKappaLoss). You can learn more about this metric [here](https://www.sciencedirect.com/science/article/abs/pii/S0167865517301666). \n\n> This metric implementation is a bit different than the competition metric, which was resolved by @taichiuemura in [here](https://www.kaggle.com/code/taichiuemura/aes-2-0-kerasnlp-starter/#%F0%9F%93%8F-%7C-Metric).","metadata":{"papermill":{"duration":0.015272,"end_time":"2024-06-09T09:36:51.375863","exception":false,"start_time":"2024-06-09T09:36:51.360591","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Result Summary","metadata":{"papermill":{"duration":0.014857,"end_time":"2024-06-09T09:36:51.406124","exception":false,"start_time":"2024-06-09T09:36:51.391267","status":"completed"},"tags":[]}},{"cell_type":"code","source":"y_true = []\ny_true.extend(valid_df['label'])\ny_true = np.array(y_true)\npreds = np.vstack(preds)\nprint('sklearn metric:', cohen_kappa_score(\n    np.sum(y_true > 0.5, axis = 1),\n    np.sum(preds > 0.5, axis = 1),\n    weights = 'quadratic',\n))\n","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.039353,"end_time":"2024-06-09T09:36:51.460461","exception":false,"start_time":"2024-06-09T09:36:51.421108","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T05:21:24.758595Z","iopub.status.idle":"2024-07-02T05:21:24.759206Z","shell.execute_reply.started":"2024-07-02T05:21:24.758915Z","shell.execute_reply":"2024-07-02T05:21:24.758939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 🧪 | Testing\n\nIn this section, we will visually test how our model performs on some samples from the validation data.\n\n> Note that we are converting the ordinal regression model outputs with `sum`, unlike a typical classification problem where we would use `argmax`.","metadata":{"papermill":{"duration":0.015113,"end_time":"2024-06-09T09:36:51.491052","exception":false,"start_time":"2024-06-09T09:36:51.475939","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Format predictions and true answers\npred_scores = np.sum((preds > 0.5).astype(int), axis=-1)\ntrue_scores = valid_df.score.values\n\n# Check 5 Predictions\nprint(\"# Predictions\\n\")\nfor i in range(5):\n    row = valid_df.iloc[i]\n    text = row.full_text\n    pred_answer = pred_scores[i]\n    true_answer = true_scores[i]\n    print(f\"❓ Text {i+1}:\\n{text[:150]} .... {text[-150:]}\\n\")\n    print(f\"✅ True: {true_answer}\\n\")\n    print(f\"🤖 Predicted: {pred_answer}\\n\")\n    print(\"-\" * 90, \"\\n\")\n","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.027354,"end_time":"2024-06-09T09:36:51.533545","exception":false,"start_time":"2024-06-09T09:36:51.506191","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T05:21:24.761257Z","iopub.status.idle":"2024-07-02T05:21:24.761851Z","shell.execute_reply.started":"2024-07-02T05:21:24.761535Z","shell.execute_reply":"2024-07-02T05:21:24.761557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 📬 | Submission\n\nIn this section, we will infer our model on the test data and then finally prepare the submission file.","metadata":{"papermill":{"duration":0.015325,"end_time":"2024-06-09T09:36:51.565431","exception":false,"start_time":"2024-06-09T09:36:51.550106","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Build Test Dataset","metadata":{"papermill":{"duration":0.015339,"end_time":"2024-06-09T09:36:51.596119","exception":false,"start_time":"2024-06-09T09:36:51.58078","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#Test Data\ntest_df = pd.read_csv(f\"{BASE_PATH}/test.csv\")\n\ntest_texts = test_df.full_text.fillna(\"\").tolist()  # Extract test texts\n\n# Build test dataset\ntest_preds = bert_finetune.predict(test_texts)\n","metadata":{"papermill":{"duration":0.074937,"end_time":"2024-06-09T09:36:51.686224","exception":false,"start_time":"2024-06-09T09:36:51.611287","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T05:21:24.763521Z","iopub.status.idle":"2024-07-02T05:21:24.764518Z","shell.execute_reply.started":"2024-07-02T05:21:24.764172Z","shell.execute_reply":"2024-07-02T05:21:24.764198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference on Test Data","metadata":{"papermill":{"duration":0.015553,"end_time":"2024-06-09T09:36:51.718406","exception":false,"start_time":"2024-06-09T09:36:51.702853","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Do inference\ntest_preds = bert_finetune.predict(test_texts)\ntest_preds = np.vstack(test_preds)\n# Convert probabilities to class labels\ntest_preds = np.sum((test_preds>0.5).astype(int), axis=-1).clip(1, 6)","metadata":{"papermill":{"duration":0.05553,"end_time":"2024-06-09T09:36:51.789469","exception":false,"start_time":"2024-06-09T09:36:51.733939","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T05:21:24.766508Z","iopub.status.idle":"2024-07-02T05:21:24.767133Z","shell.execute_reply.started":"2024-07-02T05:21:24.766832Z","shell.execute_reply":"2024-07-02T05:21:24.766857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Submission File","metadata":{"papermill":{"duration":0.015327,"end_time":"2024-06-09T09:36:51.82048","exception":false,"start_time":"2024-06-09T09:36:51.805153","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Create a DataFrame to store the submission\nsub_df = test_df[[\"essay_id\"]].copy()\n\n# Add the formatted predictions to the submission DataFrame\nsub_df[\"score\"] = test_preds\n\n# Save Submission\nsub_df.to_csv('submission.csv',index=False)\n\n# Display the first 2 rows of the submission DataFrame\nsub_df.head()","metadata":{"papermill":{"duration":0.032306,"end_time":"2024-06-09T09:36:51.868265","exception":false,"start_time":"2024-06-09T09:36:51.835959","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T05:21:24.768971Z","iopub.status.idle":"2024-07-02T05:21:24.769581Z","shell.execute_reply.started":"2024-07-02T05:21:24.769252Z","shell.execute_reply":"2024-07-02T05:21:24.769278Z"},"trusted":true},"execution_count":null,"outputs":[]}]}